---
math: true
layout: post
title: "[ì¶”ì²œì‹œìŠ¤í…œ ë…¼ë¬¸ ë¦¬ë·°] Wide & Deep Learning for Recommender Systems"
date: 2024-01-04 14:22 +0900
categories:
  - Papers
tags:
  - Recommender Systems
  - Machine Learning
  - Deep Learning
  - Linear Algebra
  - Industrial Machine Learning
  - Google
---

### Prerequisites

- Factorization Machines

### Context

- **Factorization Machine(2010)**ì´ ì„ í˜• ëª¨ë¸ê³¼ Matrix Factorizationì˜ ê²°í•©ì´ë¼ë©´, **Wide&Deep(2016)**ì˜ ê²½ìš°ëŠ” ì„ í˜• ëª¨ë¸ê³¼ ë”¥ëŸ¬ë‹ì˜ ê²°í•©ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.
- ì´í›„ Wide&Deepì˜ ì»¨ì…‰ì„ ê³„ìŠ¹í•˜ê³  ê°•í™”í•œ **DeepFM(2017)**ì´ ë“±ì¥í•œë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì„ í˜•ëª¨ë¸ê³¼ ë”¥ëŸ¬ë‹ì˜ ê²°í•©ì´ë¼ëŠ” ì»¨ì…‰ì€ ìœ ì§€ë˜ì§€ë§Œ, ì´ë¦„ì²˜ëŸ¼ ë”¥ëŸ¬ë‹ì˜ ìš”ì†Œê°€ ê°•í™”ë˜ì—ˆë‹¤.
- ì´í›„ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ì¶”ì²œ ì‹œë„ëŠ” **Neural Collaborative Filtering(2017)** ìœ¼ë¡œ ì´ì–´ì§€ê²Œ ëœë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” Wide&Deep ì—ì„œ ì„ ë³´ì¸ ë”¥ëŸ¬ë‹ê³¼ ì„ í˜•ëª¨ë¸ì˜ ê²°í•©ì´ ì•„ë‹Œ, ìˆœìˆ˜í•œ ë”¥ëŸ¬ë‹ìœ¼ë¡œ ì¶”ì²œì„ ì‹œë„í•˜ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤.

### Contributions

- ë…¼ë¬¸ì—ì„œëŠ” Memorizationê³¼ Generalizationì´ë¼ëŠ” ì²™ë„ë¥¼ ì •ì˜í•œë‹¤.
    - **Memorization**ì€ ê¸°ì¡´ì— ì‚¬ìš©ìê°€ ì¢‹ì•„í–ˆë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ê³„ì† ì¶”ì²œí•´ì£¼ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆìœ¼ë©°,
    - **Generalization**ì€ ë‘ ê°œ ì´ìƒì˜ ê³¼ê±° ë°ì´í„°ë¥¼ ê²°í•©í•´ í•™ìŠµë°ì´í„°ì— ì—†ëŠ” ìƒˆë¡œìš´ ìƒí’ˆì„ ì¶”ì²œí•˜ëŠ” ê²ƒì´ë‹¤.
- ë³¸ ë…¼ë¬¸ì—ì„œ ì„ ë³´ì´ëŠ” Wide&Deepì€ ë‘ Partë¡œ êµ¬ì„±ëœë‹¤. í•´ë‹¹ ëª¨ë¸ì—ì„œëŠ” ì„ í˜• ëª¨ë¸ì¸ Wide Partë¡œ Memorizationì„ í™•ë³´í•˜ê³ , ì ì¬ìš”ì†Œë¥¼ í•™ìŠµí•  ê°€ëŠ¥ì„±ì´ ìˆëŠ” Deep Partë¡œ Generalizationì„ í™•ë³´í•˜ê³ ì í•œë‹¤.

### Proposed Model

![Untitled](/assets/images/papers/recsys/3-wide-and-deep/1.png)

- **ì…ë ¥ ë²¡í„°**
    
    ë³¸ ë…¼ë¬¸ì—ì„œ ì…ë ¥ ë²¡í„°ëŠ” ${\bf x} =[x_1, \dots, x_n]$ë¡œ í‘œì‹œë˜ë©°, Factorization Machinesì—ì„œì™€ ê°™ì´ ê°ì¢… Feature ë°ì´í„°ë¥¼ í¬í•¨í•  ìˆ˜ ìˆë‹¤. Wide Partì™€ Deep Partì— ì–´ë–¤ Featureë¥¼ í˜ë ¤ë³´ë‚¼ì§€ëŠ” ì„¤ê³„ì— ë‹¬ë ¸ì§€ë§Œ, í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” Wide Partì— **Cross-Product Transforms**ë¥¼, Deep Partì—ëŠ” Dense Featuresì™€ Categorical Featuresë¥¼ íˆ¬ì…í•˜ì˜€ë‹¤.
    
    {: .highlight }
    > \\
    > ğŸ’¡ Cross-Product Transforms
    > - ë…¼ë¬¸ì—ì„œ ê³„ì† ì–¸ê¸‰ë˜ëŠ” CPTëŠ” Feature Engineeringì„ í†µí•´ ë½‘ì•„ë‚¸ Featureë“¤ì„ ê°€ë¦¬í‚¨ë‹¤ê³  ì´í•´í•˜ë©´ ëœë‹¤. ê°€ë ¹ $ \text{(Genre==SF)&(yearâ‰¤2000)} $ ì™€ ê°™ì€ CPTë¥¼ ì ìš©í–ˆì„ ê²½ìš°, *ìŠ¤íƒ€ì›Œì¦ˆ*ëŠ” 1ì˜ ê°’ì„ ê°€ì§€ê²Œ ë˜ê³ , *ë¡œë§ˆì˜ íœ´ì¼*ì€ 0ì˜ ê°’ì„ ê°€ì§€ê²Œ ëœë‹¤.
    > - ê²°ê³¼ì ìœ¼ë¡œ í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë°©ì‹ì„ ì±„íƒí•œ ëª¨ë¸ë¡œ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£¨ì–´ëƒˆì§€ë§Œ, ì¸ê°„ì„ í†µí•œ Feature Engineeringì´ í•„ìš”í•˜ë‹¤ëŠ” ì•½ì ì´ ìƒê²¼ë‹¤. ì´ê²ƒì€ í™”ì›¨ì´ì—ì„œ ë°œí‘œí•œ ì •ì‹ ì  í›„ì† ë…¼ë¬¸ DeepFMì—ì„œ ë³´ì™„ëœë‹¤.
    
- **Wide Part**
    
    ì…ë ¥ë²¡í„° $ {\bf x_{\text{wide}}} =[x_{\text{wide}\_1}, \dots, x_{\text{wide}\_n}] $ì— ëŒ€í•œ ì„ í˜• ëª¨ë¸ë¡œ, $y={\bf w_{\text{wide}}^{\it \text{T}} x_{\text{wide}}}+b$ë¡œ ì ì„ ìˆ˜ ìˆë‹¤. ${\bf w_{\text{wide}}}=[w_{\text{wide}\_1}, \dots, w_{\text{wide}\_n}]$ê³¼ $b\in\mathbb{R}$ì„ íŒŒë¼ë¯¸í„°ë¡œ ê°€ì§„ë‹¤. ìµœì¢… ì¶œë ¥ê°’ì€ ìŠ¤ì¹¼ë¼ê°’ì´ë‹¤.
    
- **Deep Part**
    
    ì…ë ¥ë²¡í„° ${\bf x_{\text{deep}}} =[x_{\text{deep}\_1}, \dots, x_{\text{deep}\_n}]$ì— ëŒ€í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ, í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ Fully-Connected Layerë¥¼ ì—°ì†ì ìœ¼ë¡œ ì´ì–´ë¶™ì¸ êµ¬ì¡°ì´ë‹¤. $l+1$ë²ˆì§¸ LayerëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.
    
    $$
    \begin{align*}
    a^{(l+1)} &= f(W^{(l)}a^{(l)}+b^{(l)})\\
    a^{(l)}&=\text{activation at layer }{\it l}\\b^{(l)}&=\text{bias at layer }{\it l}\\W^{(l)}&=\text{weights at layer }{\it l}\\f&=\text{activation function (e.g. ReLU)}\end{align*}
    $$
    
    ë§ˆì§€ë§‰ ReLUì˜ ì¶œë ¥ê°’ì€ ì„ í˜•ëª¨ë¸ì„ ê±°ì³ ìŠ¤ì¹¼ë¼ê°’ìœ¼ë¡œ ë³€í™˜ëœë‹¤. ì´ëŠ” $y={\bf w_{\text{deep}}^{\it \text{ T}} {\it a}^{({\it l}\_{final})}}+b$ë¡œ ì ì„ ìˆ˜ ìˆìœ¼ë©°, ${\bf w_\{\text{deep}}}=[w_{\text{deep}\_1}, \dots, w\_{\text{deep}\_n}]$ì„ íŒŒë¼ë¯¸í„°ë¡œ ê°€ì§„ë‹¤. ìµœì¢… ì¶œë ¥ê°’ì€ ìŠ¤ì¹¼ë¼ê°’ì´ë‹¤.
    
    {: .highlight }
    > \\
    > ğŸ’¡ **Handling Categorical Features**\\
    ëª¨ë“  Featureê°€ Denseí•˜ë‹¤ë©´ ì¢‹ê² ì§€ë§Œ í˜„ì‹¤ì€ ê·¸ë ‡ì§€ ì•Šë‹¤. Categorical Featuresì˜ ê²½ìš° ëŠ” 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì§€ë©°, ëŒ€ë‹¤ìˆ˜ í•­ëª©ì´ 0ì¸ Sparse Binary Vectorì´ë‹¤. ì´ì— ëŒ€ì‘í•´ ë…¼ë¬¸ì—ì„œëŠ” ê° Categorical Featureë§ˆë‹¤ Embedding ë ˆì´ì–´ë¥¼ ë‘ì–´, Sparse Featureë¥¼ Dense Embeddingsìœ¼ë¡œ ë°”ê¾¸ê³ , ëª¨ë“  Embeddingê³¼ Dense Featuresë¥¼ concatí•˜ì—¬ ìƒˆë¡œìš´ ì…ë ¥ ë²¡í„°ë¥¼ ë§Œë“ ë‹¤.
    
- **Combining Wide & Deep Outputs**
    
    Wide&Deepì˜ ìµœì¢… predictorëŠ” Wide Partì™€ Deep Partì˜ ì¶œë ¥ê°’ì˜ í•©ìœ¼ë¡œ êµ¬ì„±ëœë‹¤. 
    
    $$
    P(Y=1|{\bf x}) = \sigma({\bf w}^{T}_\text{wide}[{\bf x}, \phi({\bf x})]+{\bf w}^{T}_\text{deep}a^{(l_{final})}+b)
    $$
    
    ë§ˆì§€ë§‰ìœ¼ë¡œ Labelì€ ì‚¬ìš©ì í–‰ë™ì„ ë‚˜íƒ€ë‚¸ë‹¤. ê°€ë ¹ ë…¼ë¬¸ì—ì„œ ì§„í–‰í•œ Google Play Storeì—ì„œì˜ ì‹¤í—˜ì—ì„œëŠ” ì‚¬ìš©ìê°€ ì•±ì„ í´ë¦­í–ˆëŠ”ì§€ì˜ ì—¬ë¶€ë¥¼ 0ê³¼ 1ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ Labelë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤. Labelì´ ì´ì§„ë³€ìˆ˜ì´ë¯€ë¡œ, Predictor ì—­ì‹œ 0ê³¼ 1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜í•´ ì£¼ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ì´ ë•Œë¬¸ì— Logistic Sigmoidê°€ ìˆ˜ì‹ì— ë“¤ì–´ê°€ê²Œ ëœë‹¤.
    
- **Model Learning**
    
    ìµœì¢… Predictorì™€ Labelë¡œë¶€í„° ì‚°ì¶œëœ **Logistic Loss**ë¥¼ ì´ìš©í•˜ì—¬ Wide Partì™€ Deep Partë¥¼ ë™ì‹œì— í•™ìŠµí•œë‹¤. ì•™ìƒë¸” ë°©ë²•ê³¼ëŠ” ì°¨ì´ê°€ ìˆìŒì„ ìœ ì˜í•˜ì. Deep PartëŠ” ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµí•˜ë©°, Wide PartëŠ” **FTRL(Follow the Regularized Leader)** ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµí•œë‹¤.